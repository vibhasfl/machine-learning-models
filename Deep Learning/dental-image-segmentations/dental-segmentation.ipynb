{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f51ee89-4747-4949-9ccf-28f672ad8b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models\n",
    "import tf_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52e91ef5-43bc-45dc-b2d0-d549e5b1e07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU details:  {'device_name': 'METAL'}\n"
     ]
    }
   ],
   "source": [
    "devices = tf.config.list_physical_devices()\n",
    "print(\"\\nDevices: \", devices)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  details = tf.config.experimental.get_device_details(gpus[0])\n",
    "  print(\"GPU details: \", details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8a51017-16b3-4352-89c6-3ea4c80b0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Tensorflow Verions: {}\\n,\\nGPU : {}\".format(tf.__version__,tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2887252a-324c-43f4-9b15-23143bcbaca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Disable GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39816a2b-2bda-4ec2-8fbf-c9db6d9c8317",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_CUT_DIR = \"dental-dataset/images_cut\"\n",
    "LABEL_CUT_DIR = \"dental-dataset/labels_cut\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a56ca01-1d1f-4a59-88be-d058ff791bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_cut_path = sorted(glob(os.path.join(IMAGE_CUT_DIR,\"*.png\")))\n",
    "label_cut_path = sorted(glob(os.path.join(LABEL_CUT_DIR,\"*.png\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3825c9c6-66b1-4401-945a-60175b03276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(2):\n",
    "#     plt.figure(figsize=(8,4))\n",
    "\n",
    "#     plt.subplot(1,2,1)\n",
    "#     plt.imshow(cv2.imread(image_cut_path[i]),cmap='gray')\n",
    "#     plt.title(\"X-ray Image\")\n",
    "    \n",
    "#     plt.subplot(1,2,2)\n",
    "#     plt.imshow(cv2.imread(label_cut_path[i]),cmap='gray')\n",
    "#     plt.title(\"Segmentation Mask (Caries)\")\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57b915a0-3096-483f-bf38-4970fa3ae77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(image_cut_path) == len(label_cut_path), \"Mismatch between images and masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d0ee66c-2701-45e7-961b-ea8f01d311d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, val_images, train_label, val_label = train_test_split(image_cut_path,label_cut_path,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d0066eb-ba17-418d-b497-84768087be92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dental-dataset/images_cut/1092.png'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b981612b-5fe8-448c-8e33-f1b8ed724af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserving aspect ratio is more important in segmentation than in classification because we want object locations to remain accurate.\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 128\n",
    "\n",
    "def load_img_and_mask(image_path,label_path):\n",
    "    \n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image,channels=3)\n",
    "    image = tf.image.resize(image,[IMG_HEIGHT,IMG_WIDTH])\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize\n",
    "\n",
    "    labeled_image = tf.io.read_file(label_path)\n",
    "    labeled_image = tf.image.decode_png(labeled_image,channels=1)\n",
    "    labeled_image = tf.image.resize(labeled_image,[IMG_HEIGHT,IMG_WIDTH],method=\"nearest\")\n",
    "    labeled_image = tf.cast(labeled_image > 0, tf.uint8)  # Normalize\n",
    "\n",
    "    return image,labeled_image\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bdf87c8-f218-4ff2-8860-bba5607a5875",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "def create_dataset(data):\n",
    "    images,labels = data\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images,labels))\n",
    "    dataset = dataset.map(load_img_and_mask,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(buffer_size=5)\n",
    "    dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c27bc948-2647-40f7-93a6-72ae1209766b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 18:38:46.418041: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2025-04-22 18:38:46.418058: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
      "2025-04-22 18:38:46.418062: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.00 GB\n",
      "2025-04-22 18:38:46.418079: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-04-22 18:38:46.418087: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = create_dataset((train_images,train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bc76722-4846-41cb-9723-5ad2b1bc59cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = create_dataset((val_images,val_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b98a7474-fd16-4e49-903d-d7d6ac58f4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for images, masks in train_dataset.take(1):\n",
    "#     print(\"Image batch shape:\", images.shape[0])\n",
    "#     print(\"Mask batch shape:\", masks.shape[0])\n",
    "#     plt.subplot(2,2,1)\n",
    "#     plt.imshow(tf.keras.utils.array_to_img(images[0]))\n",
    "#     plt.subplot(2,2,2)\n",
    "#     plt.imshow(tf.keras.utils.array_to_img(masks[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bba3ed72-31ee-4ca6-aac3-657ce4514acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet(input_shape):\n",
    "    inputs = tf.keras.Input(input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    c1 = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    # Bottleneck\n",
    "    b1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(p2)\n",
    "    b1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(b1)\n",
    "\n",
    "    # Decoder\n",
    "    u1 = layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(b1)\n",
    "    u1 = layers.concatenate([u1, c2])\n",
    "    c3 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(u1)\n",
    "    c3 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c3)\n",
    "\n",
    "    u2 = layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c3)\n",
    "    u2 = layers.concatenate([u2, c1])\n",
    "    c4 = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(u2)\n",
    "    c4 = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(c4)\n",
    "\n",
    "    # Output\n",
    "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(c4)\n",
    "\n",
    "    model = model.Model(inputs, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c64d75f-2c1f-4dce-918d-d3a53946fc00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f877557-577e-41f7-8544-b2cbb69b6996",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'model' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_unet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mIMG_HEIGHT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMG_WIDTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[0;32mIn[17], line 31\u001b[0m, in \u001b[0;36mbuild_unet\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Output\u001b[39;00m\n\u001b[1;32m     29\u001b[0m outputs \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)(c4)\n\u001b[0;32m---> 31\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mModel(inputs, outputs)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'model' referenced before assignment"
     ]
    }
   ],
   "source": [
    "model = build_unet(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a500e4-f51e-4548-a7de-030632d5e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173b8686-b9a3-4d9e-a24a-6feff521bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "trained_model = model.fit(train_dataset,\n",
    "                    validation_data=val_dataset,\n",
    "                    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ee33b-4139-4b6f-9110-8e4e4ad049d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.save('models/dental-segmentation2.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fec407-7e7e-425b-9511-95688d5cada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acurracy(stats):\n",
    "    epochs = range(1, len(stats['loss']) + 1)\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, stats[\"accuracy\"], label = \"Train accuracy\")\n",
    "    plt.plot(epochs, stats[\"val_accuracy\"], label = \"Val accuracy\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, stats[\"loss\"], label = \"Train Loss\")\n",
    "    plt.plot(epochs, stats[\"val_loss\"], label = \"Val Loss\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Loss')\n",
    "    \n",
    "    plt.show()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e0934f-dbff-46d5-88f5-4b9ddcd508fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_acurracy(trained_model.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163a23b6-f614-4397-b941-30124d616e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb220b61-4edc-4834-b55b-728f8c2ad97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = tf_keras.models.load_model(\"models/dental-segmentation.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea73215f-8489-42ce-93e7-e265aaee4c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_prediction(image, ground_truth_mask, predicted_mask):\n",
    "    \"\"\"Helper to show image, actual mask, predicted mask side by side\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Original image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Ground truth mask\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"True Mask\")\n",
    "    plt.imshow(ground_truth_mask[:, :, 0], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Predicted mask\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.imshow(predicted_mask[:, :, 0], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def predict_and_visualize(model, image, true_mask):\n",
    "    \"\"\"Runs prediction and visualizes the result\"\"\"\n",
    "    image = tf.expand_dims(image, axis=0)  # (1, height, width, 3)\n",
    "    prediction = model.predict(image)[0]   # Remove batch dimension\n",
    "    \n",
    "    # Threshold predicted mask to make it binary\n",
    "    predicted_mask = tf.where(prediction > 0.5, 1.0, 0.0).numpy()\n",
    "\n",
    "    display_prediction(image[0].numpy(), true_mask.numpy(), predicted_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2da954-5d6b-4be8-8762-f521effbf3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (image, mask) in enumerate(val_dataset.take(2)):\n",
    "#     print(f\"Prediction for validation image #{i + 1}\")\n",
    "#     predict_and_visualize(model, image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e221bc-fda7-40c4-9ee7-7972105f55a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
